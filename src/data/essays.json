[
  {
    "id": "1",
    "slug": "why-automations-rot",
    "title": "Why most AI automations rot after week two",
    "date": "December 2025",
    "readTime": "5 min read",
    "summary": "The silent decay of unowned systems, and what it takes to make automation survive contact with reality.",
    "content": "There's a pattern I see over and over. A team builds an AI automation. It works in the demo. Everyone celebrates. Two weeks later, it's quietly broken and no one knows why.\n\nThis isn't a technical problem. It's an ownership problem.\n\nMost automations are built by people who won't be around when they fail. A contractor ships the MVP. An internal team moves to the next project. The automation keeps running, sort of, but no one is watching when it starts to drift.\n\nAnd they always drift. LLMs hallucinate differently as the inputs change. APIs update their schemas. Edge cases emerge that no one anticipated. The system that worked perfectly in November is silently failing by January.\n\nThe fix isn't better testing or more documentation. It's simpler: someone has to own what happens when the automation doesn't behave.\n\nThis means designing for failure from the start. Not 'how do we prevent failures' but 'what happens when this fails at 3am?' If you can't answer that question, you don't have a production system. You have a demo with a timer on it.\n\nThe automations that survive are the ones where failure modes are explicit. Where there's a human in the loop at the right moments. Where state is durable and recoverable. Where someone gets paged when things break.\n\nThis isn't glamorous work. But it's the difference between automation that actually works and automation that just looks like it works."
  },
  {
    "id": "2",
    "slug": "the-missing-owner",
    "title": "The missing owner in AI workflows",
    "date": "November 2025",
    "readTime": "7 min read",
    "summary": "Why most AI projects fail not from bad technology, but from nobody owning the outcome.",
    "content": "Every failed AI project I've seen has the same root cause. It's not the model. It's not the data. It's that no one owned what happened after the demo.\n\nHere's how it usually goes. A team identifies a process to automate. They build something clever with LLMs. It works well enough to show stakeholders. Everyone signs off and moves on.\n\nSix months later, the automation is either abandoned or actively causing problems. When you dig in, you find the same pattern: the people who built it aren't the people who have to live with it.\n\nThis is the ownership gap. In traditional software, the engineers who build a system usually maintain it. They feel the pain of their design decisions. Bad architecture means late-night pages. This feedback loop forces improvement.\n\nAI workflows break this loop. They're often built by specialists—ML engineers, data scientists, consultants—who hand off the result to operations teams. The operators don't understand the system well enough to fix it. The builders have moved on. The system rots.\n\nThe fix is structural. Whoever builds the automation should also own its operation, at least initially. They should be on the pager. They should feel the consequences of their choices.\n\nThis sounds obvious, but it cuts against how most organizations work. Building is high-status. Operations is not. Engineers want to build new things, not babysit old ones.\n\nBut in AI workflows, the building is the easy part. The hard part is keeping the thing running as reality changes around it. That's where the actual work is. And that work requires an owner."
  },
  {
    "id": "3",
    "slug": "llms-dont-fail-loudly",
    "title": "LLMs don't fail loudly — workflows do",
    "date": "October 2025",
    "readTime": "6 min read",
    "summary": "Why retry logic, wait states, and human checkpoints matter more than prompt engineering.",
    "content": "There's a fixation in AI discourse on prompts. How to write them. How to optimize them. Entire careers built on prompt engineering.\n\nMeanwhile, the workflows wrapping these prompts are held together with duct tape.\n\nHere's the problem. When an LLM fails, it usually fails quietly. It returns something that looks plausible but is subtly wrong. The calling code doesn't know anything went wrong. It just keeps going, now with corrupted data.\n\nBy the time anyone notices, the damage has propagated through the system. You're not debugging one failure—you're untangling a cascade.\n\nThe solution isn't better prompts. It's better workflows.\n\nDurable execution means your workflow can crash and resume without losing state. Retry logic means transient failures don't become permanent ones. Wait states mean humans can intervene at critical moments. Explicit checkpoints mean you know exactly where things went wrong.\n\nThis is boring infrastructure work. It's not as exciting as fine-tuning models or crafting clever prompts. But it's what separates demos from production systems.\n\nI've seen teams spend months optimizing their prompts while running everything on a cron job with no error handling. When it inevitably breaks, they have no way to recover. They just run the whole thing again and hope.\n\nThe irony is that good workflow infrastructure makes the AI parts work better. When you can reliably retry, you can use more aggressive models. When you have human checkpoints, you can catch hallucinations before they cause damage. When state is durable, you can experiment without fear.\n\nStop obsessing over prompts. Start obsessing over what happens when those prompts don't work."
  }
]
